import matplotlib.pyplot as plt
import nltk
from nltk import sent_tokenize, word_tokenize
from nltk.stem.snowball import SnowballStemmer
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import stopwords
import pandas as pd
import numpy as np
  
data = pd.read_csv("text_processing.csv")
data['totalwords'] = data['text'].str.split().str.len()
data['wordpersecond'] = data['totalwords'] / data['Duration']

is_Speaker1 = data['Speaker'] == 	"Speaker 1:"
learner_data = data[is_Speaker1]
learner_data
is_Speaker2 = data['Speaker'] == 	"Speaker 2:"
tutor_data = data[is_Speaker2]
tutor_data

plt.scatter('Duration','totalwords',c='DarkBlue',data=learner_data)
plt.scatter('Duration','totalwords',c='DarkBlue',data=tutor_data)

s = ""
for i in learner_data.text:
    s += i
s = s.lower()
import string
translate_table = dict((ord(char), None) for char in string.punctuation)   
s = s.translate(translate_table)

tokens = [t for t in s.split()]

from nltk.corpus import stopwords 
sr= stopwords.words('english') 
clean_tokens = tokens[:] 
for token in tokens: 
    if token in stopwords.words('english'): 
        clean_tokens.remove(token) 
freq = nltk.FreqDist(clean_tokens) 
for key,val in freq.items(): 
    print(str(key) + ':' + str(val)) 
freq.plot(20, cumulative=False)

s = ""
for i in tutor_data.text:
    s += i
s = s.lower()
import string
translate_table = dict((ord(char), None) for char in string.punctuation)   
s = s.translate(translate_table)

tokens = [t for t in s.split()]

from nltk.corpus import stopwords 
sr= stopwords.words('english') 
clean_tokens = tokens[:] 
for token in tokens: 
    if token in stopwords.words('english'): 
        clean_tokens.remove(token) 
freq = nltk.FreqDist(clean_tokens) 
for key,val in freq.items(): 
    print(str(key) + ':' + str(val)) 
freq.plot(20, cumulative=False)
